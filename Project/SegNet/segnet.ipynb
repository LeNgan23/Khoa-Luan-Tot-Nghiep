{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11361084,"sourceType":"datasetVersion","datasetId":6711261}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/thanhngan123/segnet?scriptVersionId=233855724\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"ver 8 (6), v10","metadata":{}},{"cell_type":"markdown","source":"# **Import lib**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport os, PIL\nimport PIL.Image\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport pandas as pd\n\nimport cv2, json\nfrom tqdm import tqdm \nimport matplotlib.pyplot as plt\n\nimport random\nfrom scipy.spatial import ConvexHull\n\n# thư viện cần để chạy code nhduong\nfrom skimage.transform import resize\nfrom skimage.io import imread\nimport time\nfrom tensorflow.keras.layers import (\n    Conv2D, BatchNormalization, Activation, MaxPooling2D,\n    UpSampling2D, Reshape, Permute\n)\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split \n\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:42:25.866818Z","iopub.execute_input":"2025-04-14T01:42:25.867112Z","iopub.status.idle":"2025-04-14T01:42:48.409632Z","shell.execute_reply.started":"2025-04-14T01:42:25.867092Z","shell.execute_reply":"2025-04-14T01:42:48.408849Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# **Chuẩn bị tập dữ liệu**","metadata":{}},{"cell_type":"code","source":"# Đọc file Excel\nfile_path = '/kaggle/input/btxrd-data/classification.xlsx'\ndf = pd.read_excel(file_path)\n\n# Hiển thị 10 dòng đầu tiên\ndf.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:42:48.410596Z","iopub.execute_input":"2025-04-14T01:42:48.411164Z","iopub.status.idle":"2025-04-14T01:42:50.076409Z","shell.execute_reply.started":"2025-04-14T01:42:48.41114Z","shell.execute_reply":"2025-04-14T01:42:50.075656Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"    image_id  center  age gender   neoplasm    tumor_type img_type  \\\n0  IMG000001       1   48      F  malignant      other mt  frontal   \n1  IMG000002       1   12      M  malignant  osteosarcoma  frontal   \n2  IMG000003       1   12      M  malignant  osteosarcoma  lateral   \n3  IMG000004       1   52      M  malignant  osteosarcoma  frontal   \n4  IMG000005       1   52      M  malignant  osteosarcoma  oblique   \n5  IMG000006       1   62      M  malignant  osteosarcoma  frontal   \n6  IMG000007       1   62      M  malignant  osteosarcoma  lateral   \n7  IMG000008       1    6      M  malignant  osteosarcoma  frontal   \n8  IMG000009       1    6      M  malignant  osteosarcoma  frontal   \n9  IMG000010       1   16      M  malignant  osteosarcoma  frontal   \n\n  skeletal_type bones_type                                             points  \\\n0        pelvis   hip bone  [[[2027.9285714285713, 1219.9285714285713], [2...   \n1    lower limb      tibia  [[[895.6756756756756, 614.1981981981984], [895...   \n2    lower limb      tibia  [[[811.7117117117116, 729.5135135135137], [831...   \n3    lower limb       foot  [[[1302.6698113207547, 1734.9056603773583], [1...   \n4    lower limb       foot  [[[1294.2446043165467, 1594.1870503597122], [1...   \n5    lower limb      femur  [[[591.9130434782609, 1916.3043478260868], [47...   \n6    lower limb      femur  [[[868.3636363636363, 420.45454545454544], [92...   \n7    upper limb    humerus  [[[585.0, 904.7058823529412], [577.35294117647...   \n8    upper limb    humerus  [[[671.6216216216216, 1090.9819819819818], [64...   \n9    upper limb    humerus  [[[327.1296296296297, 1343.1604938271603], [36...   \n\n     area  image_area  tumor_ratio is_type  image_filename  \n0  294967   7701561.0         3.83    test  IMG000001.jpeg  \n1  131041   4972480.0         2.64   train  IMG000002.jpeg  \n2  120650   6900832.0         1.75   train  IMG000003.jpeg  \n3   91065   7214337.0         1.26   train  IMG000004.jpeg  \n4   88469   7017152.0         1.26   train  IMG000005.jpeg  \n5  806550   5627824.0        14.33   train  IMG000006.jpeg  \n6  592080   5172384.0        11.45   train  IMG000007.jpeg  \n7   61063   2650000.0         2.30   train  IMG000008.jpeg  \n8   49362   3085000.0         1.60   train  IMG000009.jpeg  \n9  130881   4866640.0         2.69   train  IMG000010.jpeg  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>center</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>neoplasm</th>\n      <th>tumor_type</th>\n      <th>img_type</th>\n      <th>skeletal_type</th>\n      <th>bones_type</th>\n      <th>points</th>\n      <th>area</th>\n      <th>image_area</th>\n      <th>tumor_ratio</th>\n      <th>is_type</th>\n      <th>image_filename</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>IMG000001</td>\n      <td>1</td>\n      <td>48</td>\n      <td>F</td>\n      <td>malignant</td>\n      <td>other mt</td>\n      <td>frontal</td>\n      <td>pelvis</td>\n      <td>hip bone</td>\n      <td>[[[2027.9285714285713, 1219.9285714285713], [2...</td>\n      <td>294967</td>\n      <td>7701561.0</td>\n      <td>3.83</td>\n      <td>test</td>\n      <td>IMG000001.jpeg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>IMG000002</td>\n      <td>1</td>\n      <td>12</td>\n      <td>M</td>\n      <td>malignant</td>\n      <td>osteosarcoma</td>\n      <td>frontal</td>\n      <td>lower limb</td>\n      <td>tibia</td>\n      <td>[[[895.6756756756756, 614.1981981981984], [895...</td>\n      <td>131041</td>\n      <td>4972480.0</td>\n      <td>2.64</td>\n      <td>train</td>\n      <td>IMG000002.jpeg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>IMG000003</td>\n      <td>1</td>\n      <td>12</td>\n      <td>M</td>\n      <td>malignant</td>\n      <td>osteosarcoma</td>\n      <td>lateral</td>\n      <td>lower limb</td>\n      <td>tibia</td>\n      <td>[[[811.7117117117116, 729.5135135135137], [831...</td>\n      <td>120650</td>\n      <td>6900832.0</td>\n      <td>1.75</td>\n      <td>train</td>\n      <td>IMG000003.jpeg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>IMG000004</td>\n      <td>1</td>\n      <td>52</td>\n      <td>M</td>\n      <td>malignant</td>\n      <td>osteosarcoma</td>\n      <td>frontal</td>\n      <td>lower limb</td>\n      <td>foot</td>\n      <td>[[[1302.6698113207547, 1734.9056603773583], [1...</td>\n      <td>91065</td>\n      <td>7214337.0</td>\n      <td>1.26</td>\n      <td>train</td>\n      <td>IMG000004.jpeg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>IMG000005</td>\n      <td>1</td>\n      <td>52</td>\n      <td>M</td>\n      <td>malignant</td>\n      <td>osteosarcoma</td>\n      <td>oblique</td>\n      <td>lower limb</td>\n      <td>foot</td>\n      <td>[[[1294.2446043165467, 1594.1870503597122], [1...</td>\n      <td>88469</td>\n      <td>7017152.0</td>\n      <td>1.26</td>\n      <td>train</td>\n      <td>IMG000005.jpeg</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>IMG000006</td>\n      <td>1</td>\n      <td>62</td>\n      <td>M</td>\n      <td>malignant</td>\n      <td>osteosarcoma</td>\n      <td>frontal</td>\n      <td>lower limb</td>\n      <td>femur</td>\n      <td>[[[591.9130434782609, 1916.3043478260868], [47...</td>\n      <td>806550</td>\n      <td>5627824.0</td>\n      <td>14.33</td>\n      <td>train</td>\n      <td>IMG000006.jpeg</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>IMG000007</td>\n      <td>1</td>\n      <td>62</td>\n      <td>M</td>\n      <td>malignant</td>\n      <td>osteosarcoma</td>\n      <td>lateral</td>\n      <td>lower limb</td>\n      <td>femur</td>\n      <td>[[[868.3636363636363, 420.45454545454544], [92...</td>\n      <td>592080</td>\n      <td>5172384.0</td>\n      <td>11.45</td>\n      <td>train</td>\n      <td>IMG000007.jpeg</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>IMG000008</td>\n      <td>1</td>\n      <td>6</td>\n      <td>M</td>\n      <td>malignant</td>\n      <td>osteosarcoma</td>\n      <td>frontal</td>\n      <td>upper limb</td>\n      <td>humerus</td>\n      <td>[[[585.0, 904.7058823529412], [577.35294117647...</td>\n      <td>61063</td>\n      <td>2650000.0</td>\n      <td>2.30</td>\n      <td>train</td>\n      <td>IMG000008.jpeg</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>IMG000009</td>\n      <td>1</td>\n      <td>6</td>\n      <td>M</td>\n      <td>malignant</td>\n      <td>osteosarcoma</td>\n      <td>frontal</td>\n      <td>upper limb</td>\n      <td>humerus</td>\n      <td>[[[671.6216216216216, 1090.9819819819818], [64...</td>\n      <td>49362</td>\n      <td>3085000.0</td>\n      <td>1.60</td>\n      <td>train</td>\n      <td>IMG000009.jpeg</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>IMG000010</td>\n      <td>1</td>\n      <td>16</td>\n      <td>M</td>\n      <td>malignant</td>\n      <td>osteosarcoma</td>\n      <td>frontal</td>\n      <td>upper limb</td>\n      <td>humerus</td>\n      <td>[[[327.1296296296297, 1343.1604938271603], [36...</td>\n      <td>130881</td>\n      <td>4866640.0</td>\n      <td>2.69</td>\n      <td>train</td>\n      <td>IMG000010.jpeg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"image_dir = \"/kaggle/input/btxrd-data/BTXRD/BTXRD/images\"\nannotation_dir = \"/kaggle/input/btxrd-data/BTXRD/BTXRD/Annotations\"\n\nclassification_file_path = '/kaggle/input/btxrd-data/classification.xlsx'\ntrain_file_path = '/kaggle/input/btxrd-data/train.xlsx'\nval_file_path = '/kaggle/input/btxrd-data/val.xlsx'\ntest_file_path = '/kaggle/input/btxrd-data/test.xlsx'\n\nprocessed_image_dir = \"/kaggle/working/btxrd_v2/images\" \nannotation_dir = \"/kaggle/working/btxrd_v2/Annotations\" ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:42:50.078118Z","iopub.execute_input":"2025-04-14T01:42:50.078629Z","iopub.status.idle":"2025-04-14T01:42:50.082622Z","shell.execute_reply.started":"2025-04-14T01:42:50.078604Z","shell.execute_reply":"2025-04-14T01:42:50.081692Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df_class = pd.read_excel(classification_file_path)\n\ntumor_types = df_class['tumor_type'].dropna().unique()\n\nsummary = {}\n\nfor tumor in tumor_types:\n    mask = df_class['tumor_type'] == tumor\n    counts = df_class[mask].groupby('is_type').size()\n\n    summary[tumor] = {\n        'train': int(counts.get('train', 0)),\n        'val': int(counts.get('val', 0)),\n        'test': int(counts.get('test', 0)),\n    }\n\ndf_summary = pd.DataFrame.from_dict(summary, orient='index')\ndf_summary.index.name = 'Tumor Type'\n\ndf_summary['total'] = df_summary.sum(axis=1)\n\nprint(df_summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:42:50.084029Z","iopub.execute_input":"2025-04-14T01:42:50.084375Z","iopub.status.idle":"2025-04-14T01:42:51.062525Z","shell.execute_reply.started":"2025-04-14T01:42:50.084344Z","shell.execute_reply":"2025-04-14T01:42:51.061551Z"}},"outputs":[{"name":"stdout","text":"                          train  val  test  total\nTumor Type                                       \nother mt                     29   11     5     45\nosteosarcoma                221   53    23    297\nsynovial osteochondroma      35   13     2     50\ngiant cell tumor             65   20     8     93\nsimple bone cyst            153   37    16    206\nother bt                     86   22     7    115\nosteochondroma              547  138    69    754\nmultiple osteochondromas    178   66    19    263\nosteofibroma                 23   14     7     44\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# **Tiền xử lý dữ liệu**\nScale factor: 0.1, Target size: 512x512","metadata":{}},{"cell_type":"code","source":"SCALE_FACTOR = 0.1 # Scale về 10%\nTARGET_SIZE = 512","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:42:51.063559Z","iopub.execute_input":"2025-04-14T01:42:51.063876Z","iopub.status.idle":"2025-04-14T01:42:51.067453Z","shell.execute_reply.started":"2025-04-14T01:42:51.063847Z","shell.execute_reply":"2025-04-14T01:42:51.066761Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"output_dir = \"/kaggle/working/btxrd_v2\"\noutput_image_dir = os.path.join(output_dir, \"images\")\noutput_anno_dir = os.path.join(output_dir, \"Annotations\")\n\nos.makedirs(output_image_dir, exist_ok=True)\nos.makedirs(output_anno_dir, exist_ok=True)\n\nMAX_VISUALIZATIONS = 5 # Số lượng ảnh tối đa để trực quan hóa\nvisualized_count = 0\n\ndef get_bounding_box(points):\n    if not points:\n        return None\n    points_array = np.array(points)\n    xmin = int(np.min(points_array[:, 0]))\n    ymin = int(np.min(points_array[:, 1]))\n    xmax = int(np.max(points_array[:, 0]))\n    ymax = int(np.max(points_array[:, 1]))\n    # Đảm bảo tọa độ không âm\n    xmin = max(0, xmin)\n    ymin = max(0, ymin)\n    return (xmin, ymin, xmax, ymax)\n\ntry:\n    image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n    total_images = len(image_files)\n    if total_images == 0:\n        print(f\"Không tìm thấy file ảnh nào trong: {image_dir}\")\n        exit()\n    print(f\"Tìm thấy {total_images} ảnh để xử lý.\")\nexcept FileNotFoundError:\n    print(f\"Lỗi: Không tìm thấy thư mục ảnh: {image_dir}\")\n    exit()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:42:51.068116Z","iopub.execute_input":"2025-04-14T01:42:51.068388Z","iopub.status.idle":"2025-04-14T01:42:51.134184Z","shell.execute_reply.started":"2025-04-14T01:42:51.068367Z","shell.execute_reply":"2025-04-14T01:42:51.13343Z"}},"outputs":[{"name":"stdout","text":"Tìm thấy 3746 ảnh để xử lý.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"for file in image_files:\n    img_path = os.path.join(image_dir, file)\n    anno_filename = file.rsplit('.', 1)[0] + '.json'\n    anno_path = os.path.join(annotation_dir, anno_filename)\n\n    img_orig = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    if img_orig is None:\n        # print(f\"Không thể đọc ảnh: {file}\") # Có thể bỏ comment nếu muốn thấy lỗi cụ thể trong log tqdm\n        continue\n    orig_height, orig_width = img_orig.shape[:2]\n\n    # Đọc annotation gốc (nếu có)\n    annotation_orig = None\n    has_annotation = os.path.exists(anno_path)\n    if has_annotation:\n        try:\n            with open(anno_path, \"r\", encoding=\"utf-8\") as f:\n                annotation_orig = json.load(f)\n        except Exception as e:\n            # print(f\"Lỗi khi đọc annotation {anno_filename}: {e}\") # Bỏ comment nếu cần debug\n            has_annotation = False\n\n    # Chuẩn bị cho trực quan hóa (NẾU cần)\n    img_to_draw_orig = None\n    img_to_draw_padded = None\n    original_bboxes = []\n    transformed_bboxes = []\n\n    should_visualize = has_annotation and (visualized_count < MAX_VISUALIZATIONS)\n\n    if should_visualize:\n        img_to_draw_orig = cv2.cvtColor(img_orig, cv2.COLOR_GRAY2BGR)\n        if annotation_orig and \"shapes\" in annotation_orig:\n             for shape in annotation_orig[\"shapes\"]:\n                if \"points\" in shape and shape[\"points\"]:\n                    bbox = get_bounding_box(shape[\"points\"])\n                    if bbox:\n                        original_bboxes.append(bbox)\n                        cv2.rectangle(img_to_draw_orig, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 0, 255), 2) # Red\n\n    # Resize ảnh \n    new_width = max(1, int(orig_width * SCALE_FACTOR))\n    new_height = max(1, int(orig_height * SCALE_FACTOR))\n    img_resized = cv2.resize(img_orig, (new_width, new_height), interpolation=cv2.INTER_AREA)\n\n    # thêm padding\n    pad_h = TARGET_SIZE - new_height\n    pad_w = TARGET_SIZE - new_width\n    top = max(0, pad_h // 2)\n    bottom = max(0, pad_h - top)\n    left = max(0, pad_w // 2)\n    right = max(0, pad_w - left)\n    padded_img = cv2.copyMakeBorder(img_resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=0) \n\n    # Lưu ảnh đã xử lý\n    output_img_path = os.path.join(output_image_dir, file)\n    try:\n        cv2.imwrite(output_img_path, padded_img)\n    except Exception as e:\n        # print(f\"Lỗi khi lưu ảnh {output_img_path}: {e}\") # Bỏ comment nếu cần debug\n        continue\n\n    # Xử lý và lưu annotation (nếu có)\n    if has_annotation and annotation_orig:\n        annotation_new = json.loads(json.dumps(annotation_orig)) # Deep copy\n        if \"shapes\" in annotation_new:\n            for shape in annotation_new[\"shapes\"]:\n                if \"points\" in shape and shape[\"points\"]:\n                    original_points = shape[\"points\"]\n                    new_points_transformed = []\n                    for x, y in original_points:\n                        new_x = (x * SCALE_FACTOR) + left\n                        new_y = (y * SCALE_FACTOR) + top\n                        new_points_transformed.append([new_x, new_y])\n                    shape[\"points\"] = new_points_transformed\n\n                    if should_visualize:\n                        new_bbox = get_bounding_box(new_points_transformed)\n                        if new_bbox:\n                            transformed_bboxes.append(new_bbox)\n\n            annotation_new[\"imageWidth\"] = TARGET_SIZE\n            annotation_new[\"imageHeight\"] = TARGET_SIZE\n\n            output_annotation_path = os.path.join(output_anno_dir, anno_filename)\n            try:\n                with open(output_annotation_path, \"w\", encoding=\"utf-8\") as f:\n                    json.dump(annotation_new, f, indent=4)\n            except Exception as e:\n                pass \n\n            # Trực quan hóa (NẾU cần và đã chuẩn bị)\n            if should_visualize and img_to_draw_orig is not None:\n                img_to_draw_padded = cv2.cvtColor(padded_img, cv2.COLOR_GRAY2BGR)\n                for bbox in transformed_bboxes:\n                     cv2.rectangle(img_to_draw_padded, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2) # Green\n\n                fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n                axes[0].imshow(img_to_draw_orig)\n                axes[0].set_title(f'Original: {file}\\nSize: {orig_width}x{orig_height}')\n                axes[0].axis('off')\n                axes[1].imshow(img_to_draw_padded)\n                axes[1].set_title(f'Processed\\nSize: {TARGET_SIZE}x{TARGET_SIZE}')\n                axes[1].axis('off')\n                plt.suptitle(f\"Visualization {visualized_count + 1}/{MAX_VISUALIZATIONS}\")\n                plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n                plt.show()\n                visualized_count += 1\n\nprint(f\"Ảnh và annotation đã xử lý được lưu tại: {output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:42:51.135095Z","iopub.execute_input":"2025-04-14T01:42:51.135406Z","iopub.status.idle":"2025-04-14T01:44:34.231033Z","shell.execute_reply.started":"2025-04-14T01:42:51.135377Z","shell.execute_reply":"2025-04-14T01:44:34.230193Z"}},"outputs":[{"name":"stdout","text":"Ảnh và annotation đã xử lý được lưu tại: /kaggle/working/btxrd_v2\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Kiểm tra xem khi xoay ảnh mark có xoay không\nprocessed_image_dir = \"/kaggle/working/btxrd_v2/images\" \nannotation_dir = \"/kaggle/working/btxrd_v2/Annotations\" \n\nrotation_angle = 90  \nnum_visualizations = 5 \npoint_color_orig = 'red'  \npoint_color_rotated = 'lime' \npoint_size = 15 \n\nprint(f\"Kiểm tra xoay ảnh và điểm annotation từ JSON:\")\nprint(f\"Thư mục ảnh: {processed_image_dir}\")\nprint(f\"Thư mục annotation: {annotation_dir}\")\nprint(f\"Góc xoay kiểm tra: {rotation_angle} độ\")\n\n# lấy danh sách file ảnh\ntry:\n    all_image_files = [f for f in os.listdir(processed_image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n    if not all_image_files:\n        print(f\"Lỗi: Không tìm thấy file ảnh nào trong {processed_image_dir}\")\n        exit()\n    print(f\"\\nTìm thấy tổng cộng {len(all_image_files)} ảnh.\")\nexcept FileNotFoundError:\n    print(f\"Lỗi: Không tìm thấy thư mục ảnh: {processed_image_dir}\")\n    exit()\nexcept Exception as e:\n    print(f\"Lỗi khi truy cập thư mục ảnh: {e}\")\n    exit()\n\n# Chọn ngẫu nhiên ảnh để kiểm tra \n# Lọc ra những ảnh có file annotation tương ứng tồn tại\nfiles_with_annotations = []\nfor img_file in all_image_files:\n    json_filename = os.path.splitext(img_file)[0] + '.json'\n    json_path = os.path.join(annotation_dir, json_filename)\n    if os.path.exists(json_path):\n        files_with_annotations.append(img_file)\n\nif not files_with_annotations:\n    print(\"Lỗi: Không tìm thấy cặp ảnh/annotation nào để kiểm tra.\")\n    exit()\n\nnum_to_select = min(num_visualizations, len(files_with_annotations))\nif num_to_select < num_visualizations:\n     print(f\"Cảnh báo: Chỉ tìm thấy {num_to_select} ảnh có annotation, sẽ hiển thị những ảnh này.\")\n\nselected_files = random.sample(files_with_annotations, num_to_select)\nprint(f\"Đã chọn ngẫu nhiên {num_to_select} ảnh có annotation để hiển thị kiểm tra xoay.\")\n\n# kiểm tra và hiển thị\nfor i, filename in enumerate(selected_files):\n    image_path = os.path.join(processed_image_dir, filename)\n    json_filename = os.path.splitext(filename)[0] + '.json'\n    json_path = os.path.join(annotation_dir, json_filename)\n\n    print(f\"\\n--- Ảnh {i+1}/{num_to_select}: {filename} ---\")\n    print(f\"  Annotation: {json_path}\")\n\n    img = cv2.imread(image_path, cv2.IMREAD_COLOR) \n    if img is None:\n        print(f\"  Lỗi: Không thể đọc ảnh {image_path}\")\n        continue\n    h_img, w_img = img.shape[:2]\n\n    annotation = None\n    original_points_flat = [] # Danh sách [[x1,y1], [x2,y2], ...]\n    try:\n        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n            annotation = json.load(f)\n        # Trích xuất tất cả các điểm từ tất cả các shape\n        if annotation and \"shapes\" in annotation:\n            for shape in annotation[\"shapes\"]:\n                if \"points\" in shape and shape[\"points\"]:\n                    original_points_flat.extend(shape[\"points\"])\n        if not original_points_flat:\n            print(\"  Cảnh báo: Không tìm thấy 'points' hợp lệ trong file JSON. Bỏ qua hiển thị điểm.\")\n\n    except FileNotFoundError:\n         print(f\"  Lỗi: Không tìm thấy file annotation {json_path}\")\n         continue \n    except Exception as e:\n        print(f\"  Lỗi khi đọc hoặc phân tích JSON {json_path}: {e}\")\n        continue # Bỏ qua nếu lỗi đọc json\n\n    # xoay\n    center = (w_img // 2, h_img // 2)\n    rotation_matrix = cv2.getRotationMatrix2D(center, rotation_angle, 1.0)\n\n    img_rotated = cv2.warpAffine(img, rotation_matrix, (w_img, h_img),\n                                 flags=cv2.INTER_LINEAR,\n                                 borderMode=cv2.BORDER_CONSTANT,\n                                 borderValue=(0, 0, 0))\n\n    transformed_points_flat = []\n    if original_points_flat:\n        for x, y in original_points_flat:\n            new_x = rotation_matrix[0, 0] * x + rotation_matrix[0, 1] * y + rotation_matrix[0, 2]\n            new_y = rotation_matrix[1, 0] * x + rotation_matrix[1, 1] * y + rotation_matrix[1, 2]\n            transformed_points_flat.append([new_x, new_y])\n\n    fig, axes = plt.subplots(1, 2, figsize=(15, 7)) # 1 hàng, 2 cột\n    \n    # vẽ cho ảnh gốc\n    axes[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    axes[0].set_title(f\"Original Image\\n{filename}\")\n    axes[0].axis('off')\n    if annotation and \"shapes\" in annotation:\n        plotted_orig = False\n        for shape in annotation[\"shapes\"]:\n            if \"points\" in shape and len(shape[\"points\"]) >= 3: # Cần ít nhất 3 điểm để tạo Convex Hull\n                points_orig_np = np.array(shape[\"points\"])\n                try:\n                    hull = ConvexHull(points_orig_np)\n                    # Vẽ các cạnh của Convex Hull\n                    for simplex in hull.simplices:\n                        axes[0].plot(points_orig_np[simplex, 0], points_orig_np[simplex, 1],\n                                     color=point_color_orig, linewidth=2,\n                                     label='Original Outline' if not plotted_orig else \"\")\n                        plotted_orig = True # Đảm bảo label chỉ được thêm một lần cho ảnh đầu tiên\n                except Exception as e:\n                    print(f\"  Lỗi khi tính Convex Hull cho ảnh gốc shape: {e}\")\n                    # Dự phòng: Vẽ các điểm nếu không tính được hull\n                    axes[0].scatter(points_orig_np[:, 0], points_orig_np[:, 1], color=point_color_orig, s=5, label='Original Points (Hull Error)' if not plotted_orig else \"\")\n                    plotted_orig = True\n    \n        if plotted_orig:\n            axes[0].legend(loc='upper right')\n    \n    # vẽ cho ảnh đã xoay\n    axes[1].imshow(cv2.cvtColor(img_rotated, cv2.COLOR_BGR2RGB))\n    axes[1].set_title(f\"Rotated Image & Outline ({rotation_angle}°)\")\n    axes[1].axis('off')\n    if annotation and \"shapes\" in annotation:\n        plotted_rotated = False\n        for shape_index, shape in enumerate(annotation[\"shapes\"]):\n             if \"points\" in shape and len(shape[\"points\"]) >= 3:\n                original_points_shape = shape[\"points\"]\n                transformed_points_shape = []\n                for x, y in original_points_shape:\n                    new_x = rotation_matrix[0, 0] * x + rotation_matrix[0, 1] * y + rotation_matrix[0, 2]\n                    new_y = rotation_matrix[1, 0] * x + rotation_matrix[1, 1] * y + rotation_matrix[1, 2]\n                    transformed_points_shape.append([new_x, new_y])\n    \n                if transformed_points_shape:\n                    points_transformed_np = np.array(transformed_points_shape)\n                    try:\n                        if len(points_transformed_np) >= 3:\n                             hull_rotated = ConvexHull(points_transformed_np)\n                             for simplex in hull_rotated.simplices:\n                                 axes[1].plot(points_transformed_np[simplex, 0], points_transformed_np[simplex, 1],\n                                              color=point_color_rotated, linewidth=2,\n                                              label='Rotated Outline' if not plotted_rotated else \"\")\n                                 plotted_rotated = True\n                        elif len(points_transformed_np) > 0 : # Nếu ít hơn 3 điểm, vẽ điểm rời rạc\n                            axes[1].scatter(points_transformed_np[:, 0], points_transformed_np[:, 1], color=point_color_rotated, s=5, label='Rotated Points (<3)' if not plotted_rotated else \"\")\n                            plotted_rotated = True\n    \n                    except Exception as e:\n                        print(f\"  Lỗi khi tính Convex Hull cho ảnh xoay shape: {e}\")\n                        # Dự phòng: Vẽ các điểm nếu không tính được hull\n                        if len(points_transformed_np) > 0:\n                             axes[1].scatter(points_transformed_np[:, 0], points_transformed_np[:, 1], color=point_color_rotated, s=5, label='Rotated Points (Hull Error)' if not plotted_rotated else \"\")\n                             plotted_rotated = True\n    \n        if plotted_rotated:\n            axes[1].legend(loc='upper right')\n    \n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:44:34.233613Z","iopub.execute_input":"2025-04-14T01:44:34.233832Z","iopub.status.idle":"2025-04-14T01:44:34.284578Z","shell.execute_reply.started":"2025-04-14T01:44:34.233814Z","shell.execute_reply":"2025-04-14T01:44:34.283807Z"}},"outputs":[{"name":"stdout","text":"Kiểm tra xoay ảnh và điểm annotation từ JSON:\nThư mục ảnh: /kaggle/working/btxrd_v2/images\nThư mục annotation: /kaggle/working/btxrd_v2/Annotations\nGóc xoay kiểm tra: 90 độ\n\nTìm thấy tổng cộng 3746 ảnh.\nLỗi: Không tìm thấy cặp ảnh/annotation nào để kiểm tra.\nCảnh báo: Chỉ tìm thấy 0 ảnh có annotation, sẽ hiển thị những ảnh này.\nĐã chọn ngẫu nhiên 0 ảnh có annotation để hiển thị kiểm tra xoay.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# **Load data**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nimport os\nimport json\nimport cv2\nimport time\n\n# --- Configuration ---\nN_CLASSES = 2 # Background, Cancerous\nBATCH_SIZE = 8\nIMG_SIZE = (512, 512) # Target image size (height, width)\nSEED = 42 # For reproducibility\n\n# --- Paths ---\nBASE_DATA_DIR = '/kaggle/input/btxrd-data/' # Adapt if needed\nEXCEL_PATH = os.path.join(BASE_DATA_DIR, 'classification.xlsx')\nPROCESSED_IMAGE_DIR = '/kaggle/working/btxrd_v2/images' # Adapt if needed\nANNOTATION_DIR = '/kaggle/working/btxrd_v2/Annotations' # Adapt if needed\nMODEL_SAVE_PATH = 'unet_segmentation_best.keras'\n\n# Create directories if they don't exist (optional, good practice)\nos.makedirs(PROCESSED_IMAGE_DIR, exist_ok=True)\nos.makedirs(ANNOTATION_DIR, exist_ok=True)\n\n# --- Load Master Metadata ---\nprint(\"Loading metadata from:\", EXCEL_PATH)\ntry:\n    classification_df = pd.read_excel(EXCEL_PATH)\n    print(f\"Loaded classification_df: {len(classification_df)} entries\")\n    # Ensure 'image_id' is string and doesn't contain extensions\n    classification_df['image_id'] = classification_df['image_id'].astype(str).str.replace(r'\\.(jpg|jpeg|png)$', '', regex=True)\n    print(\"Unique is_type values:\", classification_df['is_type'].unique())\nexcept FileNotFoundError:\n    print(f\"Error: Excel file not found at {EXCEL_PATH}\")\n    exit() # Or handle appropriately\nexcept Exception as e:\n    print(f\"Error reading Excel file: {e}\")\n    exit()\n\n# --- Calculate Mean/Std for Standardization (Requirement 3) ---\n# This requires iterating through the training images once beforehand.\n# For simplicity in this example, we'll stick to [0, 1] normalization,\n# but here's where you'd calculate it if needed.\n# train_image_paths_for_stats = [os.path.join(PROCESSED_IMAGE_DIR, f\"{img_id}.png\") # or .jpg/.jpeg\n#                                for img_id in classification_df[classification_df['is_type'] == 'train']['image_id']]\n# # Add logic here to load images, calculate mean/std pixel values across the training set\n# GLOBAL_MEAN = ... # Calculated mean pixel value(s)\n# GLOBAL_STD = ...  # Calculated std dev pixel value(s)\n# print(f\"Calculated Mean: {GLOBAL_MEAN}, Std: {GLOBAL_STD}\") # Placeholder\n\n# --- Augmentation Function (Requirement 1) ---\ndef augment(image, mask):\n    # Geometric Augmentations (apply to both image and mask)\n    if tf.random.uniform(()) > 0.5:\n        image = tf.image.flip_left_right(image)\n        mask = tf.image.flip_left_right(mask)\n\n    if tf.random.uniform(()) > 0.5:\n        image = tf.image.flip_up_down(image)\n        mask = tf.image.flip_up_down(mask)\n\n    # Rotation\n    k = tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)\n    image = tf.image.rot90(image, k)\n    # Ensure mask rotation doesn't mess up one-hot encoding if applied later\n    # Apply rotation *before* one-hot encoding the mask in parse_fn\n\n    # Photometric Augmentations (apply only to image)\n    image = tf.image.random_brightness(image, max_delta=0.2)\n    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n    # Ensure image values remain in a valid range after augmentation\n    image = tf.clip_by_value(image, 0.0, 1.0) # Important if using [0,1] normalization\n\n    return image, mask\n\n# --- Data Loader Function (Requirements 2, 5, 6) ---\ndef data_loader(image_ids, is_training, batch_size, img_size, n_classes):\n    \"\"\"\n    Creates a tf.data.Dataset pipeline for loading images and masks.\n\n    Args:\n        image_ids (list or np.array): List of image base names (without extension).\n        is_training (bool): Whether this is the training set (enables shuffle, repeat, augment).\n        batch_size (int): Batch size.\n        img_size (tuple): Target image size (height, width).\n        n_classes (int): Number of segmentation classes.\n\n    Returns:\n        tf.data.Dataset: Configured dataset pipeline.\n    \"\"\"\n    dataset = tf.data.Dataset.from_tensor_slices(image_ids)\n\n    if is_training:\n        dataset = dataset.shuffle(len(image_ids), seed=SEED) # Shuffle indices\n\n    def parse_fn(image_id_tensor):\n        image_id = tf.strings.as_string(image_id_tensor) # Get string from tensor\n\n        # Construct potential paths (handle multiple extensions if needed)\n        # Prioritize png, then jpeg, then jpg - adjust based on your actual data\n        img_path_png = tf.strings.join([PROCESSED_IMAGE_DIR, '/', image_id, '.png'])\n        img_path_jpeg = tf.strings.join([PROCESSED_IMAGE_DIR, '/', image_id, '.jpeg'])\n        img_path_jpg = tf.strings.join([PROCESSED_IMAGE_DIR, '/', image_id, '.jpg'])\n\n        # Determine which path exists (tf.io.is_file or similar might be complex here)\n        # A common approach is to try loading, but tf.py_function is cleaner for this check\n        def find_image_path(img_id_str):\n            img_id_str = img_id_str.decode('utf-8')\n            for ext in ['.png', '.jpeg', '.jpg']:\n                path = os.path.join(PROCESSED_IMAGE_DIR, img_id_str + ext)\n                if os.path.exists(path):\n                    return path.encode('utf-8')\n            # Return a dummy path or raise error if no image found\n            print(f\"Warning: Image file not found for ID: {img_id_str}\")\n            # Returning one of the attempts; TF load will fail later if it doesn't exist\n            return os.path.join(PROCESSED_IMAGE_DIR, img_id_str + \".png\").encode('utf-8') # Or handle error\n\n        image_path = tf.py_function(find_image_path, [image_id], tf.string)\n        image_path.set_shape([]) # Set shape for py_function output\n\n        # Load Image\n        image = tf.io.read_file(image_path)\n        try:\n            # Try decoding as PNG first, then JPEG if needed\n            image = tf.image.decode_image(image, channels=1, expand_animations=False) # Handles png, jpg, bmp, gif\n        except tf.errors.InvalidArgumentError: # Fallback if decode_image fails (e.g., corrupted)\n             print(f\"Warning: Could not decode image {image_path}. Using zeros.\")\n             image = tf.zeros([img_size[0], img_size[1], 1], dtype=tf.uint8)\n\n\n        image = tf.image.resize(image, img_size, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n        image = tf.cast(image, tf.float32) / 255.0 # Normalize to [0, 1] (Requirement 2)\n        # If using standardization (Req 3):\n        # image = (tf.cast(image, tf.float32) - GLOBAL_MEAN) / GLOBAL_STD\n\n        # Load Mask using tf.numpy_function for complex JSON parsing\n        annotation_path = tf.strings.join([ANNOTATION_DIR, '/', image_id, '.json'])\n\n        def get_mask(annotation_path_bytes):\n            path = annotation_path_bytes.decode('utf-8')\n            # Initialize mask as background\n            mask = np.zeros(img_size, dtype=np.uint8) # Use target img_size\n            if not os.path.exists(path):\n                # print(f\"Annotation file not found: {path}. Returning zero mask.\") # Can be noisy\n                pass # Keep zero mask\n            else:\n                try:\n                    with open(path) as f:\n                        data = json.load(f)\n                    \n                    # Get original image dimensions for scaling points\n                    orig_h = data.get('imageHeight', img_size[0])\n                    orig_w = data.get('imageWidth', img_size[1])\n                    if orig_h == 0 or orig_w == 0: # Handle potential zero dimensions\n                         orig_h, orig_w = img_size[0], img_size[1]\n\n\n                    scale_y = img_size[0] / orig_h\n                    scale_x = img_size[1] / orig_w\n\n                    for shape in data.get('shapes', []):\n                        if shape.get('label', '').lower() == 'cancerous' or True: # Adjust label if needed, or take all polygons\n                           if 'points' not in shape or len(shape['points']) < 3:\n                               continue\n                           pts = np.array(shape['points'], dtype=np.float32)\n                           # Scale points to the target IMG_SIZE\n                           pts[:, 0] *= scale_x\n                           pts[:, 1] *= scale_y\n                           pts = np.round(pts).astype(np.int32)\n                           # Ensure points are within bounds after scaling/rounding\n                           pts[:, 0] = np.clip(pts[:, 0], 0, img_size[1] - 1)\n                           pts[:, 1] = np.clip(pts[:, 1], 0, img_size[0] - 1)\n                           # Fill polygon with class index 1 (assuming 0 is background)\n                           cv2.fillPoly(mask, [pts], 1)\n                except json.JSONDecodeError:\n                    print(f\"Error decoding JSON: {path}. Returning zero mask.\")\n                except Exception as e:\n                    print(f\"Error processing annotation {path}: {e}. Returning zero mask.\")\n\n            # Add channel dimension\n            mask = mask[..., np.newaxis]\n            return mask.astype(np.int32) # Return integer mask before one-hot\n\n        # Use tf.numpy_function to run the Python code\n        mask_int = tf.numpy_function(get_mask, [annotation_path], tf.int32)\n        mask_int.set_shape((*img_size, 1)) # Explicitly set shape\n\n        # --- Apply Augmentations (if training) ---\n        if is_training:\n            # Apply geometric augmentations before one-hot encoding mask\n            image, mask_int = augment(image, mask_int) # Pass integer mask to augment\n\n        # --- One-Hot Encode Mask (Requirement 5) ---\n        # Squeeze the channel dim before one-hot\n        mask = tf.one_hot(tf.squeeze(mask_int, axis=-1), depth=n_classes, dtype=tf.float32)\n        mask.set_shape((*img_size, n_classes)) # Ensure shape consistency\n\n        # Final check on image shape (sometimes resize might lose channel dim for grayscale)\n        image.set_shape((*img_size, 1))\n\n        return image, mask\n\n    # Configure dataset pipeline\n    dataset = dataset.map(parse_fn, num_parallel_calls=tf.data.AUTOTUNE)\n\n    if is_training:\n        # Repeat *before* batching for effective shuffling across epochs\n        dataset = dataset.repeat()\n\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE) # Prefetch for performance\n\n    return dataset\n\n# --- Split Data based on 'is_type' column (Requirement 6) ---\ntrain_ids = classification_df[classification_df['is_type'] == 'train']['image_id'].tolist()\nval_ids = classification_df[classification_df['is_type'] == 'val']['image_id'].tolist()\ntest_ids = classification_df[classification_df['is_type'] == 'test']['image_id'].tolist()\n\nprint(f\"Train samples: {len(train_ids)}\")\nprint(f\"Validation samples: {len(val_ids)}\")\nprint(f\"Test samples: {len(test_ids)}\")\n\n# --- Create Datasets using the data_loader ---\ntrain_ds = data_loader(train_ids, is_training=True, batch_size=BATCH_SIZE, img_size=IMG_SIZE, n_classes=N_CLASSES)\nval_ds = data_loader(val_ids, is_training=False, batch_size=BATCH_SIZE, img_size=IMG_SIZE, n_classes=N_CLASSES)\ntest_ds = data_loader(test_ids, is_training=False, batch_size=BATCH_SIZE, img_size=IMG_SIZE, n_classes=N_CLASSES)\n\n# --- Verify dataset output shapes (optional but recommended) ---\nprint(\"Checking dataset element spec...\")\nfor image_batch, mask_batch in train_ds.take(1):\n    print(\"Train Image batch shape:\", image_batch.shape) # (BATCH_SIZE, H, W, 1)\n    print(\"Train Mask batch shape:\", mask_batch.shape)   # (BATCH_SIZE, H, W, N_CLASSES)\nfor image_batch, mask_batch in val_ds.take(1):\n    print(\"Validation Image batch shape:\", image_batch.shape)\n    print(\"Validation Mask batch shape:\", mask_batch.shape)\n\n\n# --- Loss Functions and Metrics ---\ndef dice_coef(y_true, y_pred, smooth=1e-6):\n    # y_true shape: (batch, h, w, n_classes), y_pred shape: (batch, h, w, n_classes)\n    # Flatten spatial dimensions\n    y_true_f = tf.reshape(y_true, [-1, N_CLASSES])\n    y_pred_f = tf.reshape(y_pred, [-1, N_CLASSES])\n\n    # Calculate intersection and union per class\n    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=0) # Shape: (n_classes,)\n    union = tf.reduce_sum(y_true_f, axis=0) + tf.reduce_sum(y_pred_f, axis=0) # Shape: (n_classes,)\n\n    dice = (2. * intersection + smooth) / (union + smooth) # Shape: (n_classes,)\n\n    # Typically interested in the foreground class(es) Dice\n    # Option 1: Average Dice across all classes (including background)\n    # return tf.reduce_mean(dice)\n    # Option 2: Dice for the foreground class (index 1) - Common for binary\n    return dice[1] # Return Dice coefficient specifically for class 1 (cancerous)\n\ndef dice_loss(y_true, y_pred, smooth=1e-6):\n     # Uses the same logic as dice_coef but returns 1 - dice\n    y_true_f = tf.reshape(y_true, [-1, N_CLASSES])\n    y_pred_f = tf.reshape(y_pred, [-1, N_CLASSES])\n    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=0)\n    union = tf.reduce_sum(y_true_f, axis=0) + tf.reduce_sum(y_pred_f, axis=0)\n    dice_per_class = (2. * intersection + smooth) / (union + smooth)\n    # Option 1: Average loss over classes\n    # return 1.0 - tf.reduce_mean(dice_per_class)\n    # Option 2: Loss based on foreground class Dice\n    return 1.0 - dice_per_class[1] # Loss based on class 1\n\n\ndef iou_coef(y_true, y_pred, smooth=1e-6):\n    y_true_f = tf.reshape(y_true, [-1, N_CLASSES])\n    y_pred_f = tf.reshape(y_pred, [-1, N_CLASSES])\n    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=0)\n    union = tf.reduce_sum(y_true_f, axis=0) + tf.reduce_sum(y_pred_f, axis=0) - intersection\n    iou_per_class = (intersection + smooth) / (union + smooth)\n    # Option 1: Average IoU\n    # return tf.reduce_mean(iou_per_class)\n    # Option 2: Foreground IoU\n    return iou_per_class[1] # IoU for class 1\n\n\n# Focal loss implementation remains the same if desired\ndef focal_loss(gamma=2., alpha=0.25):\n    def focal_loss_fixed(y_true, y_pred):\n        y_true = tf.cast(y_true, tf.float32) # Ensure y_true is float\n        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1. - tf.keras.backend.epsilon())\n\n        # Calculate cross-entropy\n        cross_entropy = -y_true * tf.math.log(y_pred)\n\n        # Calculate focal loss weights\n        loss_weight = alpha * y_true * tf.pow((1 - y_pred), gamma)\n\n        # Apply weights to cross-entropy\n        focal_loss = loss_weight * cross_entropy\n\n        # Sum across classes and reduce mean across batch/pixels\n        # return tf.reduce_mean(tf.reduce_sum(focal_loss, axis=-1)) # Mean over pixels/batch\n        # Sum over classes, then take the mean over batch/pixels\n        return tf.reduce_mean(tf.reduce_sum(focal_loss, axis=-1))\n\n    return focal_loss_fixed\n\n# Combined Loss (Example: Dice + Focal)\ndef combined_loss(y_true, y_pred):\n     # Weighting can be adjusted (e.g., 0.5 * dice + 0.5 * focal)\n    return dice_loss(y_true, y_pred) + focal_loss(gamma=2.0, alpha=0.25)(y_true, y_pred)\n    # return dice_loss(y_true, y_pred) # Or just use Dice Loss\n\n\n# --- Build U-Net Model ---\n# The U-Net definition from the original code is kept, as it's a standard structure.\ndef unet_model(input_shape, num_classes):\n    inputs = layers.Input(input_shape)\n\n    # Contracting Path (Encoder)\n    c1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n    c1 = layers.BatchNormalization()(c1)\n    c1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c1)\n    c1 = layers.BatchNormalization()(c1)\n    p1 = layers.MaxPooling2D((2, 2))(c1) # Explicit MaxPooling\n\n    c2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(p1)\n    c2 = layers.BatchNormalization()(c2)\n    c2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c2)\n    c2 = layers.BatchNormalization()(c2)\n    p2 = layers.MaxPooling2D((2, 2))(c2)\n\n    c3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p2)\n    c3 = layers.BatchNormalization()(c3)\n    c3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c3)\n    c3 = layers.BatchNormalization()(c3)\n    p3 = layers.MaxPooling2D((2, 2))(c3)\n\n    c4 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p3)\n    c4 = layers.BatchNormalization()(c4)\n    c4 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c4)\n    c4 = layers.BatchNormalization()(c4)\n    p4 = layers.MaxPooling2D((2, 2))(c4)\n\n    # Bottleneck\n    c5 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p4)\n    c5 = layers.BatchNormalization()(c5)\n    c5 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c5)\n    c5 = layers.BatchNormalization()(c5)\n\n    # Expansive Path (Decoder)\n    u6 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c5)\n    # u6 = layers.Resizing(c4.shape[1], c4.shape[2])(u6) # Optional if shapes mismatch slightly\n    u6 = layers.Concatenate()([u6, c4])\n    c6 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u6)\n    c6 = layers.BatchNormalization()(c6)\n    c6 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c6)\n    c6 = layers.BatchNormalization()(c6)\n\n    u7 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c6)\n    # u7 = layers.Resizing(c3.shape[1], c3.shape[2])(u7)\n    u7 = layers.Concatenate()([u7, c3])\n    c7 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u7)\n    c7 = layers.BatchNormalization()(c7)\n    c7 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c7)\n    c7 = layers.BatchNormalization()(c7)\n\n    u8 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c7)\n    # u8 = layers.Resizing(c2.shape[1], c2.shape[2])(u8)\n    u8 = layers.Concatenate()([u8, c2])\n    c8 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u8)\n    c8 = layers.BatchNormalization()(c8)\n    c8 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c8)\n    c8 = layers.BatchNormalization()(c8)\n\n    u9 = layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c8)\n    # u9 = layers.Resizing(c1.shape[1], c1.shape[2])(u9)\n    u9 = layers.Concatenate()([u9, c1])\n    c9 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(u9)\n    c9 = layers.BatchNormalization()(c9)\n    c9 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c9)\n    c9 = layers.BatchNormalization()(c9)\n\n    # Output layer: Use softmax for multi-class (even if N=2 with one-hot)\n    outputs = layers.Conv2D(num_classes, (1, 1), activation='softmax')(c9)\n\n    model = models.Model(inputs=[inputs], outputs=[outputs])\n    return model\n\n# --- Instantiate and Compile Model ---\ntf.keras.backend.clear_session() # Clear previous models from memory\nmodel = unet_model(input_shape=(*IMG_SIZE, 1), num_classes=N_CLASSES)\n\n# Use combined loss or just dice_loss\n# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n#               loss=combined_loss,\n#               metrics=['accuracy', dice_coef, iou_coef])\n\n# Compile with Dice Loss only (often a good starting point)\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n              loss=dice_loss, # Use the foreground dice loss\n              metrics=['accuracy', dice_coef, iou_coef]) # Monitor foreground Dice/IoU\n\n\nmodel.summary()\n\n# --- Callbacks ---\ncheckpoint_cb = ModelCheckpoint(\n    MODEL_SAVE_PATH,\n    save_best_only=True,\n    monitor='val_dice_coef', # Monitor validation Dice coefficient for the foreground class\n    mode='max',             # Save the model with the highest validation Dice\n    verbose=1\n)\nlr_cb = ReduceLROnPlateau(\n    monitor='val_loss',     # Reduce LR if validation loss plateaus\n    factor=0.2,             # Reduce LR by factor of 5\n    patience=5,             # Number of epochs with no improvement before reducing LR\n    min_lr=1e-6,            # Minimum learning rate\n    verbose=1\n)\nearly_cb = EarlyStopping(\n    monitor='val_dice_coef', # Stop if validation Dice stops improving\n    patience=10,            # Number of epochs to wait for improvement\n    mode='max',             # Looking for maximum Dice\n    restore_best_weights=True, # Restore weights from the best epoch\n    verbose=1\n)\n\n# --- Training (Requirement 7 - Using model.fit, fulfilling other parts) ---\n# Calculate steps per epoch and validation steps\nsteps_per_epoch = len(train_ids) // BATCH_SIZE\nif steps_per_epoch == 0:\n    steps_per_epoch = 1 # Ensure at least one step even if dataset is small\nvalidation_steps = len(val_ids) // BATCH_SIZE\nif validation_steps == 0:\n    validation_steps = 1\n\nprint(f\"Steps per epoch: {steps_per_epoch}\")\nprint(f\"Validation steps: {validation_steps}\")\n\nN_EPOCHS = 50 # Set desired number of epochs\n\nprint(\"Starting training...\")\nstart_time = time.time()\n\n# Fit the model using the prepared tf.data datasets\nhistory = model.fit(\n    train_ds,\n    epochs=N_EPOCHS,\n    steps_per_epoch=steps_per_epoch,\n    validation_data=val_ds,\n    validation_steps=validation_steps,\n    callbacks=[checkpoint_cb, lr_cb, early_cb],\n    verbose=1 # Set to 1 or 2 for progress updates\n)\n\nend_time = time.time()\nprint(f\"Training finished.\")\nprint(f\"Total Training time: {(end_time - start_time)/60:.2f} minutes\")\n\n# --- Evaluation on Test Set (Optional) ---\nprint(\"Evaluating on test set...\")\n# Load the best weights saved by ModelCheckpoint\nmodel.load_weights(MODEL_SAVE_PATH)\n\ntest_steps = len(test_ids) // BATCH_SIZE\nif test_steps == 0:\n    test_steps = 1\n\ntest_loss, test_accuracy, test_dice, test_iou = model.evaluate(test_ds, steps=test_steps, verbose=1)\nprint(f\"\\nTest Results:\")\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Test Dice Coefficient (Foreground): {test_dice:.4f}\")\nprint(f\"Test IoU (Foreground): {test_iou:.4f}\")\n\n# --- Plot Training History (Optional) ---\nimport matplotlib.pyplot as plt\n\ndef plot_history(history):\n    # Plot Dice Coefficient and Validation Dice Coefficient\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['dice_coef'], label='Training Dice Coef')\n    plt.plot(history.history['val_dice_coef'], label='Validation Dice Coef')\n    plt.title('Dice Coefficient')\n    plt.xlabel('Epoch')\n    plt.ylabel('Dice')\n    plt.legend()\n    plt.grid(True)\n\n    # Plot Loss and Validation Loss\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title('Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n\n    plt.tight_layout()\n    plt.show()\n\nplot_history(history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:44:34.286189Z","iopub.execute_input":"2025-04-14T01:44:34.286483Z","iopub.status.idle":"2025-04-14T01:44:41.316716Z","shell.execute_reply.started":"2025-04-14T01:44:34.286463Z","shell.execute_reply":"2025-04-14T01:44:41.315419Z"}},"outputs":[{"name":"stdout","text":"Loading metadata from: /kaggle/input/btxrd-data/classification.xlsx\nLoaded classification_df: 3746 entries\nUnique is_type values: ['test' 'train' 'val' nan]\nTrain samples: 2677\nValidation samples: 744\nTest samples: 298\nChecking dataset element spec...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-06128807061f>\u001b[0m in \u001b[0;36m<cell line: 234>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;31m# --- Verify dataset output shapes (optional but recommended) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Checking dataset element spec...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Image batch shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (BATCH_SIZE, H, W, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Mask batch shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (BATCH_SIZE, H, W, N_CLASSES)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    777\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3084\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3086\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3087\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5982\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5983\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnknownError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Error in user-defined function passed to ParallelMapDatasetV2:2 transformation with iterator: Iterator::Root::Prefetch::FiniteTake::Prefetch::BatchV2::ForeverRepeat[0]::ParallelMapV2: AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'decode'\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 268, in __call__\n    return func(device, token, args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 146, in __call__\n    outputs = self._call(device, args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 153, in _call\n    ret = self._func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/tmp/__autograph_generated_file3myuo2m2.py\", line 23, in find_image_path\n    img_id_str = ag__.converted_call(ag__.ld(img_id_str).decode, ('utf-8',), None, fscope_1)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor.py\", line 260, in __getattr__\n    self.__getattribute__(name)\n\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'decode'\n\n\n\t [[{{node EagerPyFunc}}]] [Op:IteratorGetNext] name: "],"ename":"UnknownError","evalue":"{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Error in user-defined function passed to ParallelMapDatasetV2:2 transformation with iterator: Iterator::Root::Prefetch::FiniteTake::Prefetch::BatchV2::ForeverRepeat[0]::ParallelMapV2: AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'decode'\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 268, in __call__\n    return func(device, token, args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 146, in __call__\n    outputs = self._call(device, args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 153, in _call\n    ret = self._func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/tmp/__autograph_generated_file3myuo2m2.py\", line 23, in find_image_path\n    img_id_str = ag__.converted_call(ag__.ld(img_id_str).decode, ('utf-8',), None, fscope_1)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor.py\", line 260, in __getattr__\n    self.__getattribute__(name)\n\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'decode'\n\n\n\t [[{{node EagerPyFunc}}]] [Op:IteratorGetNext] name: ","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"abc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:44:41.317196Z","iopub.status.idle":"2025-04-14T01:44:41.317529Z","shell.execute_reply":"2025-04-14T01:44:41.317411Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"nháp từ đây tới dưới","metadata":{}},{"cell_type":"code","source":"N_CLASSES = 2\nBATCH_SIZE = 8\nIMG_SIZE = (512, 512)\n\n# Đọc dữ liệu từ Excel\nclassification_df = pd.read_excel('/kaggle/input/btxrd-data/classification.xlsx')\nval_df = pd.read_excel('/kaggle/input/btxrd-data/val.xlsx')\ntest_df = pd.read_excel('/kaggle/input/btxrd-data/test.xlsx')\n\nprocessed_image_dir = '/kaggle/working/btxrd_v2/images'\nannotation_dir = '/kaggle/working/btxrd_v2/Annotations'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:44:41.318275Z","iopub.status.idle":"2025-04-14T01:44:41.318559Z","shell.execute_reply":"2025-04-14T01:44:41.318441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hàm xác định phần mở rộng của ảnh\ndef resolve_image_extension(df, image_dir):\n    def get_filename(image_id):\n        # image_id đã bao gồm đuôi .jpeg hoặc .jpg\n        path = os.path.join(image_dir, image_id)\n        return image_id if os.path.exists(path) else None\n\n    df['image_filename'] = df['image_id'].apply(get_filename)\n    df = df[df['image_filename'].notna()].reset_index(drop=True)\n    return df\n\ntrain_df = resolve_image_extension(train_df, processed_image_dir)\nval_df = resolve_image_extension(val_df, processed_image_dir)\ntest_df = resolve_image_extension(test_df, processed_image_dir)\n\nprint(\"train_df sau khi resolve:\", len(train_df))\nprint(\"val_df sau khi resolve:\", len(val_df))\nprint(\"test_df sau khi resolve:\", len(test_df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:44:41.319508Z","iopub.status.idle":"2025-04-14T01:44:41.319763Z","shell.execute_reply":"2025-04-14T01:44:41.319657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def augment(image, mask):\n    if tf.random.uniform(()) > 0.5:\n        image = tf.image.flip_left_right(image)\n        mask = tf.image.flip_left_right(mask)\n\n    if tf.random.uniform(()) > 0.5:\n        image = tf.image.flip_up_down(image)\n        mask = tf.image.flip_up_down(mask)\n\n    k = tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)\n    image = tf.image.rot90(image, k)\n    mask = tf.image.rot90(mask, k)\n\n    # Brightness and contrast (only on image)\n    if tf.random.uniform(()) > 0.5:\n        image = tf.image.random_brightness(image, max_delta=0.2)\n    if tf.random.uniform(()) > 0.5:\n        image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n\n    return image, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:44:41.320462Z","iopub.status.idle":"2025-04-14T01:44:41.320817Z","shell.execute_reply":"2025-04-14T01:44:41.320664Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data loader\ndef data_loader(df, is_training):\n    print(\"data_loader: input df length =\", len(df))\n    def parse_fn(image_filename):\n        image_path = tf.strings.join([processed_image_dir, '/', image_filename])\n        image = tf.io.read_file(image_path)\n        image = tf.image.decode_jpeg(image, channels=1)\n        image = tf.image.resize(image, IMG_SIZE)\n        image = tf.cast(image, tf.float32) / 255.0\n\n        image_id = tf.strings.regex_replace(image_filename, r'\\.(jpg|jpeg)$', '')\n        annotation_path = tf.strings.join([annotation_dir, '/', image_id, '.json'])\n\n        # Ground Truth Mask\n        def get_mask(annotation_path_str):\n            path = annotation_path_str.decode('utf-8')\n            if not os.path.exists(path):\n                return np.zeros((*IMG_SIZE, 1), dtype=np.float32)\n            with open(path) as f:\n                data = json.load(f)\n            mask = np.zeros(IMG_SIZE, dtype=np.uint8)\n            for shape in data.get('shapes', []):\n                if 'points' not in shape:\n                    continue\n                pts = np.array(shape['points'], dtype=np.float32)\n                if pts.shape[0] < 3:\n                    continue\n                pts[:, 0] = pts[:, 0] * IMG_SIZE[1] / data['imageWidth']\n                pts[:, 1] = pts[:, 1] * IMG_SIZE[0] / data['imageHeight']\n                pts = np.round(pts).astype(np.int32)\n                cv2.fillPoly(mask, [pts], 1)\n            return mask[..., np.newaxis].astype(np.float32)\n\n        mask = tf.numpy_function(get_mask, [annotation_path], tf.float32)\n        mask.set_shape((*IMG_SIZE, 1))\n        mask = tf.cast(mask, tf.int32)\n        mask = tf.one_hot(tf.squeeze(mask), depth=N_CLASSES)\n\n         # --- Augmentation ---\n        image, mask = augment(image, mask)\n\n        return image, mask\n\n    df = df.sample(frac=1).reset_index(drop=True) if is_training else df\n    dataset = tf.data.Dataset.from_tensor_slices(df['image_filename'].values)\n    dataset = dataset.map(parse_fn, num_parallel_calls=tf.data.AUTOTUNE)\n    if is_training:\n        dataset = dataset.shuffle(100).repeat().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n    else:\n        dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:44:41.321776Z","iopub.status.idle":"2025-04-14T01:44:41.322045Z","shell.execute_reply":"2025-04-14T01:44:41.321927Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_dataset_from_index(index_list, is_training=True):\n    dataset = tf.data.Dataset.from_tensor_slices(index_list)\n\n    def load_image_mask(idx):\n        image_id = classification_df.iloc[int(idx.numpy())]['image_id']\n        return image_id.encode()\n\n    def parse_fn(image_id):\n        image_id_str = image_id.numpy().decode(\"utf-8\")\n        \n        image_path = os.path.join(processed_image_dir, f\"{image_id_str}.png\")\n        mask_path = os.path.join(annotation_dir, f\"{image_id_str}.json\")\n\n        image = load_grayscale_image(image_path)  # bạn đã có\n        mask = load_mask_from_json(mask_path)     # bạn đã có\n\n        return image, mask\n\n    def map_fn(idx):\n        image_id = tf.py_function(load_image_mask, [idx], tf.string)\n        return tf.py_function(parse_fn, [image_id], [tf.float32, tf.float32])\n\n    dataset = dataset.map(map_fn, num_parallel_calls=tf.data.AUTOTUNE)\n\n    if is_training:\n        dataset = dataset.shuffle(100).repeat()\n\n    dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n    return dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:44:41.322774Z","iopub.status.idle":"2025-04-14T01:44:41.323067Z","shell.execute_reply":"2025-04-14T01:44:41.32296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Lấy index dòng tương ứng với 'train' từ classification.xlsx\ntrain_indices = classification_df[classification_df['is_type'] == 'train'].index.tolist()\n# Tạo train dataset từ index\ntrain_ds = load_dataset_from_index(train_indices, is_training=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:44:41.323975Z","iopub.status.idle":"2025-04-14T01:44:41.324369Z","shell.execute_reply":"2025-04-14T01:44:41.324162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds = data_loader(train_df, is_training=True)\nval_ds = data_loader(val_df, is_training=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:44:41.325506Z","iopub.status.idle":"2025-04-14T01:44:41.325802Z","shell.execute_reply":"2025-04-14T01:44:41.325692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def dice_loss(y_true, y_pred, smooth=1e-6):\n    y_true_f = tf.reshape(y_true, [-1, N_CLASSES])\n    y_pred_f = tf.reshape(y_pred, [-1, N_CLASSES])\n    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=0)\n    union = tf.reduce_sum(y_true_f + y_pred_f, axis=0)\n    dice = (2. * intersection + smooth) / (union + smooth)\n    return 1 - tf.reduce_mean(dice)\n\ndef dice_coef(y_true, y_pred, smooth=1e-6):\n    y_true_f = tf.reshape(y_true, [-1, N_CLASSES])\n    y_pred_f = tf.reshape(y_pred, [-1, N_CLASSES])\n    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=0)\n    union = tf.reduce_sum(y_true_f + y_pred_f, axis=0)\n    dice = (2. * intersection + smooth) / (union + smooth)\n    return tf.reduce_mean(dice)\n\ndef iou_coef(y_true, y_pred, smooth=1e-6):\n    y_true_f = tf.reshape(y_true, [-1, N_CLASSES])\n    y_pred_f = tf.reshape(y_pred, [-1, N_CLASSES])\n    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=0)\n    union = tf.reduce_sum(y_true_f + y_pred_f - y_true_f * y_pred_f, axis=0)\n    iou = (intersection + smooth) / (union + smooth)\n    return tf.reduce_mean(iou)\n\ndef focal_loss(gamma=2., alpha=0.25):\n    def focal_loss_fixed(y_true, y_pred):\n        y_true = tf.reshape(y_true, [-1, N_CLASSES])\n        y_pred = tf.reshape(y_pred, [-1, N_CLASSES])\n        epsilon = 1e-6\n        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n        cross_entropy = -y_true * tf.math.log(y_pred)\n        weight = alpha * tf.pow(1 - y_pred, gamma)\n        loss = weight * cross_entropy\n        return tf.reduce_mean(tf.reduce_sum(loss, axis=1))\n    return focal_loss_fixed\n\ndef combined_loss(y_true, y_pred):\n    return dice_loss(y_true, y_pred) + focal_loss()(y_true, y_pred)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:44:41.326513Z","iopub.status.idle":"2025-04-14T01:44:41.326842Z","shell.execute_reply":"2025-04-14T01:44:41.326697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build U-Net model\ndef unet_model():\n    inputs = layers.Input((*IMG_SIZE, 1))\n\n    x = layers.Conv2D(32, (3, 3), strides=2, padding='same', activation='relu')(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    skip1 = x\n\n    x = layers.Conv2D(64, (3, 3), strides=2, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    skip2 = x\n\n    x = layers.Conv2D(128, (3, 3), strides=2, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    skip3 = x\n\n    x = layers.Conv2D(256, (3, 3), strides=2, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Conv2DTranspose(128, (3, 3), strides=2, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Resizing(skip3.shape[1], skip3.shape[2])(x)\n    x = layers.Concatenate()([x, skip3])\n    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Conv2DTranspose(64, (3, 3), strides=2, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Resizing(skip2.shape[1], skip2.shape[2])(x)\n    x = layers.Concatenate()([x, skip2])\n    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Conv2DTranspose(32, (3, 3), strides=2, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Resizing(skip1.shape[1], skip1.shape[2])(x)\n    x = layers.Concatenate()([x, skip1])\n    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Conv2DTranspose(32, (3, 3), strides=2, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    outputs = layers.Conv2D(N_CLASSES, (1, 1), activation='softmax')(x)\n\n    return models.Model(inputs, outputs)\n\nmodel = unet_model()\n# model.compile(optimizer='adam', loss=dice_loss, metrics=['accuracy', dice_coef, iou_coef])\nmodel.compile(optimizer='adam', loss=combined_loss, metrics=['accuracy', dice_coef, iou_coef])\n\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:44:41.327569Z","iopub.status.idle":"2025-04-14T01:44:41.327858Z","shell.execute_reply":"2025-04-14T01:44:41.327743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Callbacks\ncheckpoint_cb = ModelCheckpoint('unet_v2.keras', save_best_only=True, monitor='val_loss', mode='min')\nlr_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\nearly_cb = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\nsteps_per_epoch = max(1, len(train_df) // BATCH_SIZE)\nvalidation_steps = max(1, len(val_df) // BATCH_SIZE)\n\ntrain_ds = data_loader(train_df, is_training=True).repeat()\nval_ds = data_loader(val_df, is_training=False)\n\nstart_time = time.time()\nhistory = model.fit(\n    train_ds,\n    epochs=50,\n    validation_data=val_ds,\n    steps_per_epoch=steps_per_epoch,\n    # validation_steps=validation_steps,\n    callbacks=[checkpoint_cb, lr_cb, early_cb]\n)\nend_time = time.time()\nprint(f\"Training time: {(end_time - start_time)/60:.2f} minutes\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:44:41.328568Z","iopub.status.idle":"2025-04-14T01:44:41.328906Z","shell.execute_reply":"2025-04-14T01:44:41.32874Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"nahsp từ đầu tới đây","metadata":{}},{"cell_type":"code","source":"# Evaluate Model\nscore = model.evaluate(x_val, y_val, verbose=0)\nprint(f'Validation loss: {score[0]:.4f}')\nprint(f'Validation accuracy: {score[1]:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:44:41.329915Z","iopub.status.idle":"2025-04-14T01:44:41.330258Z","shell.execute_reply":"2025-04-14T01:44:41.330081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize Prediction \nrandom_index = np.random.randint(0, len(x_val))\ninput_image = x_val[random_index]\ntrue_mask_one_hot = y_val_orig[random_index]\n\nprediction_reshaped = model.predict(np.expand_dims(input_image, axis=0))\nprediction = prediction_reshaped.reshape((MODEL_IMG_H, MODEL_IMG_W, N_CLASSES))\npredicted_mask = np.argmax(prediction, axis=-1)\ntrue_mask = np.argmax(true_mask_one_hot, axis=-1)\n\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 3, 1)\nplt.imshow(input_image[:, :, 0], cmap='gray')\nplt.title('Input Image')\nplt.axis('off')\nplt.subplot(1, 3, 2)\nplt.imshow(true_mask, cmap='gray')\nplt.title('True Mask (Tumor=1)')\nplt.axis('off')\nplt.subplot(1, 3, 3)\nplt.imshow(predicted_mask, cmap='gray')\nplt.title('Predicted Mask (Tumor=1)')\nplt.axis('off')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:44:41.331368Z","iopub.status.idle":"2025-04-14T01:44:41.331647Z","shell.execute_reply":"2025-04-14T01:44:41.331525Z"}},"outputs":[],"execution_count":null}]}